{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongdai\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import re # use to split \"Cabin\" variable\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "train = pd.read_csv('train.csv')\n",
    "train_survived = train['Survived']\n",
    "train.drop(['PassengerId','Survived'],axis = 1, inplace = True)\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "PassengerId = test['PassengerId']\n",
    "test.drop('PassengerId',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split numerical and catigorical variable\n",
    "train_num = train.select_dtypes(exclude = [\"object\"])\n",
    "train_cat = train.select_dtypes(include = ['object'])\n",
    "\n",
    "test_num = test.select_dtypes(exclude = ['object'])\n",
    "test_cat = test.select_dtypes(include = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      0\n",
       "Age       177\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_train = Imputer(strategy=\"median\")\n",
    "imputer_train.fit(train_num)\n",
    "X_train = imputer_train.transform(train_num)\n",
    "train_num = pd.DataFrame(X_train, columns=train_num.columns,\n",
    "                                  index = list(train_num.index.values))\n",
    "\n",
    "imputer_test = Imputer(strategy=\"median\")\n",
    "imputer_test.fit(test_num)\n",
    "X_test = imputer_test.transform(test_num)\n",
    "test_num = pd.DataFrame(X_test, columns=test_num.columns,\n",
    "                                  index = list(test_num.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name          0\n",
       "Sex           0\n",
       "Ticket        0\n",
       "Cabin       687\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the title of the name, the front part of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat[\"Name_front\"] = \"None\"\n",
    "for i in range(len(train_cat)):\n",
    "    for j in train_cat.Name[i].split():\n",
    "        if \".\" in j:\n",
    "            train_cat[\"Name_front\"][i] = j\n",
    "            \n",
    "test_cat[\"Name_front\"] = \"None\"\n",
    "for i in range(len(test_cat)):\n",
    "    for j in test_cat.Name[i].split():\n",
    "        if \".\" in j:\n",
    "            test_cat[\"Name_front\"][i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr.          517\n",
       "Miss.        182\n",
       "Mrs.         124\n",
       "Master.       40\n",
       "Dr.            7\n",
       "Rev.           6\n",
       "Major.         2\n",
       "Mlle.          2\n",
       "Col.           2\n",
       "Don.           1\n",
       "Lady.          1\n",
       "Ms.            1\n",
       "Jonkheer.      1\n",
       "Capt.          1\n",
       "Sir.           1\n",
       "L.             1\n",
       "Mme.           1\n",
       "Countess.      1\n",
       "Name: Name_front, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat[\"Name_front\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr.        240\n",
       "Miss.       78\n",
       "Mrs.        72\n",
       "Master.     21\n",
       "Col.         2\n",
       "Rev.         2\n",
       "Ms.          1\n",
       "Dr.          1\n",
       "Dona.        1\n",
       "Name: Name_front, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat[\"Name_front\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will put those frequency smaller than 10 into one group\n",
    "train_rare_name_list = list(train_cat[\"Name_front\"].value_counts().index[4:])\n",
    "for i in range(len(train_cat['Name_front'])):\n",
    "    if train_cat['Name_front'][i] in train_rare_name_list:\n",
    "        train_cat['Name_front'][i] = \"Rare\"\n",
    "\n",
    "## do the same for the test set\n",
    "test_rare_name_list = list(test_cat[\"Name_front\"].value_counts().index[4:])\n",
    "for i in range(len(test_cat['Name_front'])):\n",
    "    if test_cat['Name_front'][i] in test_rare_name_list:\n",
    "        test_cat['Name_front'][i] = \"Rare\"\n",
    "\n",
    "train_cat.drop(['Name'], axis = 1, inplace = True)\n",
    "test_cat.drop(['Name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr.        517\n",
       "Miss.      182\n",
       "Mrs.       124\n",
       "Master.     40\n",
       "Rare        28\n",
       "Name: Name_front, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat[\"Name_front\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dealing with Cabin variable\n",
    "train_cat[\"Cabin_front\"] = \"None\"\n",
    "for i in range(len(train_cat[\"Cabin\"])):\n",
    "    if not pd.isnull(train_cat[\"Cabin\"][i]):\n",
    "        match = re.match(r\"([a-z]+)([0-9]+)\", train_cat[\"Cabin\"][i], re.I)\n",
    "        if match:\n",
    "            train_cat[\"Cabin_front\"][i] = match.groups()[0]\n",
    "            \n",
    "test_cat[\"Cabin_front\"] = \"None\"\n",
    "for i in range(len(test_cat[\"Cabin\"])):\n",
    "    if not pd.isnull(test_cat[\"Cabin\"][i]):\n",
    "        match = re.match(r\"([a-z]+)([0-9]+)\", test_cat[\"Cabin\"][i], re.I)\n",
    "        if match:\n",
    "            test_cat[\"Cabin_front\"][i] = match.groups()[0]\n",
    "            \n",
    "train_cat.drop([\"Cabin\"], axis = 1, inplace = True)\n",
    "test_cat.drop([\"Cabin\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None    332\n",
       "C        35\n",
       "B        18\n",
       "D        12\n",
       "E         9\n",
       "A         7\n",
       "F         4\n",
       "G         1\n",
       "Name: Cabin_front, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat[\"Cabin_front\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None    695\n",
       "C        59\n",
       "B        47\n",
       "E        32\n",
       "D        30\n",
       "A        15\n",
       "F         9\n",
       "G         4\n",
       "Name: Cabin_front, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat[\"Cabin_front\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex            0\n",
       "Ticket         0\n",
       "Embarked       2\n",
       "Name_front     0\n",
       "Cabin_front    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Embarked\" for train_cat set, using its most frequent\n",
    "train_cat = train_cat.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex            0\n",
       "Ticket         0\n",
       "Embarked       0\n",
       "Name_front     0\n",
       "Cabin_front    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex            0\n",
       "Ticket         0\n",
       "Embarked       0\n",
       "Name_front     0\n",
       "Cabin_front    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PC', 'SOTON/O.Q.', 'A/5.', 'CA', 'C.A.', 'CA.', 'W./C.']\n"
     ]
    }
   ],
   "source": [
    "## deal with ticket variable\n",
    "train_cat[\"Ticket_prep\"] = \"None\"\n",
    "for i in range(len(train_cat.Ticket)):\n",
    "    train_cat[\"Ticket_prep\"][i] = train_cat.Ticket[i].split()[0]\n",
    "    \n",
    "test_cat[\"Ticket_prep\"] = \"None\"\n",
    "for i in range(len(test_cat.Ticket)):\n",
    "    test_cat[\"Ticket_prep\"][i] = test_cat.Ticket[i].split()[0]\n",
    "    \n",
    "ticket_common_name = list(set(list(test_cat[\"Ticket_prep\"].value_counts()[:15].index)) & \n",
    "                         set(list(train_cat[\"Ticket_prep\"].value_counts()[:15].index)))\n",
    "\n",
    "print(ticket_common_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat[\"Ticket_prep\"] = \"None\"\n",
    "for i in range(len(train_cat.Ticket)):\n",
    "    if train_cat.Ticket[i].split()[0] in ticket_common_name:\n",
    "        train_cat[\"Ticket_prep\"][i] = train_cat.Ticket[i].split()[0]\n",
    "\n",
    "test_cat[\"Ticket_prep\"] = \"None\"\n",
    "for i in range(len(test_cat.Ticket)):\n",
    "    if test_cat.Ticket[i].split()[0] in ticket_common_name:\n",
    "        test_cat[\"Ticket_prep\"][i] = test_cat.Ticket[i].split()[0]\n",
    "        \n",
    "train_cat.drop([\"Ticket\"], axis = 1, inplace = True)\n",
    "test_cat.drop([\"Ticket\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None          766\n",
       "PC             60\n",
       "C.A.           27\n",
       "W./C.           9\n",
       "SOTON/O.Q.      8\n",
       "CA.             8\n",
       "A/5.            7\n",
       "CA              6\n",
       "Name: Ticket_prep, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat[\"Ticket_prep\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex            0\n",
       "Embarked       0\n",
       "Name_front     0\n",
       "Cabin_front    0\n",
       "Ticket_prep    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get rid of the outliers in the train set\n",
    "# Removie the outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(train_num[[\"Age\",'Fare']])\n",
    "y_noano = clf.predict(train_num[[\"Age\",'Fare']])\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Outlier'])\n",
    "useful_index = y_noano[y_noano['Outlier'] == 1].index.values\n",
    "\n",
    "train_num = train_num.iloc[useful_index]\n",
    "train_num.reset_index(drop = True, inplace = True)\n",
    "\n",
    "train_survived = train_survived.iloc[useful_index]\n",
    "\n",
    "train_cat = train_cat.iloc[useful_index]\n",
    "train_cat.reset_index(drop = True, inplace = True)\n",
    "\n",
    "train = train.iloc[useful_index]\n",
    "train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num.Parch[test_num.Parch > 6] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_prepared = pd.concat([pd.get_dummies(train_num[[\"Pclass\",\"SibSp\",\"Parch\"]].astype(str)),\n",
    "                                train_num[[\"Age\",'Fare']]], axis = 1)\n",
    "test_num_prepared = pd.concat([pd.get_dummies(test_num[[\"Pclass\",\"SibSp\",\"Parch\"]].astype(str)),\n",
    "                                test_num[[\"Age\",'Fare']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conbind train and test set\n",
    "train_cat_prepared = pd.get_dummies(train_cat)\n",
    "test_cat_prepared = pd.get_dummies(test_cat)\n",
    "\n",
    "train_prepared = pd.concat([train_num_prepared, train_cat_prepared],axis = 1)\n",
    "test_prepared = pd.concat([test_num_prepared, test_cat_prepared],axis = 1)\n",
    "\n",
    "train_prepared = train_prepared.reset_index(drop=True)\n",
    "test_prepared = test_prepared.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the prepared train set is (801, 45)\n",
      "the shape of the prepared test set is (418, 45)\n"
     ]
    }
   ],
   "source": [
    "print(\"the shape of the prepared train set is {}\".format(train_prepared.shape))\n",
    "print(\"the shape of the prepared test set is {}\".format(test_prepared.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared = train_prepared.values\n",
    "test_prepared = test_prepared.values\n",
    "train_survived = train_survived.values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_prepared,\n",
    "                                                      train_survived, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normal Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 45  # number of features\n",
    "n_hidden1 = 32\n",
    "n_hidden2 = 16\n",
    "n_hidden3 = 4\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.selu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.selu, name=\"hidden3\")\n",
    "    logits = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") ##\n",
    "     \n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 2001\n",
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.4878049 Validation accuracy: 0.61509436\n",
      "200 Batch accuracy: 0.85365856 Validation accuracy: 0.76981133\n",
      "400 Batch accuracy: 0.9512195 Validation accuracy: 0.76981133\n",
      "600 Batch accuracy: 0.902439 Validation accuracy: 0.76981133\n",
      "800 Batch accuracy: 0.9268293 Validation accuracy: 0.7773585\n",
      "1000 Batch accuracy: 0.9512195 Validation accuracy: 0.76603776\n",
      "1200 Batch accuracy: 0.9756098 Validation accuracy: 0.754717\n",
      "1400 Batch accuracy: 0.9756098 Validation accuracy: 0.7433962\n",
      "1600 Batch accuracy: 0.9268293 Validation accuracy: 0.7509434\n",
      "1800 Batch accuracy: 0.9512195 Validation accuracy: 0.7433962\n",
      "2000 Batch accuracy: 0.9268293 Validation accuracy: 0.7283019\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 200 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 45 \n",
    "n_hidden1 = 32\n",
    "n_hidden2 = 16\n",
    "n_hidden3 = 8\n",
    "n_outputs = 2\n",
    "\n",
    "batch_norm_momentum = 0.99\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense, #  whether we could change the order\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    hidden3 = my_dense_layer(bn1, n_hidden2, name=\"hidden3\")\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 2001\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accarcy: 0.6635514 Validation accuracy: 0.6679245\n",
      "200 Train accarcy: 0.92523366 Validation accuracy: 0.76981133\n",
      "400 Train accarcy: 0.8785047 Validation accuracy: 0.7811321\n",
      "600 Train accarcy: 0.93457943 Validation accuracy: 0.7735849\n",
      "800 Train accarcy: 0.92523366 Validation accuracy: 0.7471698\n",
      "1000 Train accarcy: 0.92523366 Validation accuracy: 0.75849056\n",
      "1200 Train accarcy: 0.95327103 Validation accuracy: 0.7509434\n",
      "1400 Train accarcy: 0.93457943 Validation accuracy: 0.7433962\n",
      "1600 Train accarcy: 0.95327103 Validation accuracy: 0.7509434\n",
      "1800 Train accarcy: 0.95327103 Validation accuracy: 0.7358491\n",
      "2000 Train accarcy: 0.94392526 Validation accuracy: 0.75849056\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_bat = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        if epoch % 200 == 0:\n",
    "            print(epoch, \"Train accarcy:\", accuracy_bat,\n",
    "                  \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add regularization\n",
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 45 \n",
    "n_hidden1 = 32\n",
    "n_hidden2 = 16\n",
    "n_hidden3 = 8\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.15  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_initializer=he_init, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\")\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden3_drop, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accarcy: 0.71910113 Validation accuracy: 0.6415094\n",
      "100 Train accarcy: 0.6853933 Validation accuracy: 0.6301887\n",
      "200 Train accarcy: 0.71348315 Validation accuracy: 0.6301887\n",
      "300 Train accarcy: 0.7022472 Validation accuracy: 0.6339623\n",
      "400 Train accarcy: 0.6741573 Validation accuracy: 0.6528302\n",
      "500 Train accarcy: 0.73595506 Validation accuracy: 0.6679245\n",
      "600 Train accarcy: 0.7022472 Validation accuracy: 0.6792453\n",
      "700 Train accarcy: 0.7303371 Validation accuracy: 0.7018868\n",
      "800 Train accarcy: 0.8258427 Validation accuracy: 0.76603776\n",
      "900 Train accarcy: 0.8258427 Validation accuracy: 0.7811321\n",
      "1000 Train accarcy: 0.7752809 Validation accuracy: 0.8037736\n",
      "1100 Train accarcy: 0.7752809 Validation accuracy: 0.79622644\n",
      "1200 Train accarcy: 0.80898875 Validation accuracy: 0.8188679\n",
      "1300 Train accarcy: 0.8146067 Validation accuracy: 0.7735849\n",
      "1400 Train accarcy: 0.7977528 Validation accuracy: 0.79622644\n",
      "1500 Train accarcy: 0.83707863 Validation accuracy: 0.75849056\n",
      "1600 Train accarcy: 0.83707863 Validation accuracy: 0.7433962\n",
      "1700 Train accarcy: 0.8707865 Validation accuracy: 0.7811321\n",
      "1800 Train accarcy: 0.8202247 Validation accuracy: 0.8301887\n",
      "1900 Train accarcy: 0.85955054 Validation accuracy: 0.8226415\n",
      "2000 Train accarcy: 0.85393256 Validation accuracy: 0.7811321\n",
      "2100 Train accarcy: 0.8146067 Validation accuracy: 0.76226413\n",
      "2200 Train accarcy: 0.8258427 Validation accuracy: 0.7924528\n",
      "2300 Train accarcy: 0.7977528 Validation accuracy: 0.78867924\n",
      "2400 Train accarcy: 0.85393256 Validation accuracy: 0.7773585\n",
      "2500 Train accarcy: 0.89325845 Validation accuracy: 0.7849057\n",
      "2600 Train accarcy: 0.83707863 Validation accuracy: 0.79622644\n",
      "2700 Train accarcy: 0.8707865 Validation accuracy: 0.79622644\n",
      "2800 Train accarcy: 0.8033708 Validation accuracy: 0.8113208\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2801\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_bat = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, \"Train accarcy:\", accuracy_bat,\n",
    "                  \"Validation accuracy:\", accuracy_val)\n",
    "    Z = logits.eval(feed_dict = {X: test_prepared})\n",
    "    y_pred =np.argmax(Z, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accarcy: 0.6416979\n",
      "200 Train accarcy: 0.6691635\n",
      "400 Train accarcy: 0.67790264\n",
      "600 Train accarcy: 0.67540574\n",
      "800 Train accarcy: 0.67540574\n",
      "1000 Train accarcy: 0.67915106\n",
      "1200 Train accarcy: 0.6828964\n",
      "1400 Train accarcy: 0.6828964\n",
      "1600 Train accarcy: 0.6866417\n",
      "1800 Train accarcy: 0.6866417\n",
      "2000 Train accarcy: 0.6841448\n",
      "2200 Train accarcy: 0.6866417\n",
      "2400 Train accarcy: 0.69538075\n",
      "2600 Train accarcy: 0.71660423\n",
      "2800 Train accarcy: 0.72659177\n",
      "3000 Train accarcy: 0.7802746\n",
      "3200 Train accarcy: 0.7852684\n",
      "3400 Train accarcy: 0.7940075\n",
      "3600 Train accarcy: 0.7977528\n",
      "3800 Train accarcy: 0.8002497\n",
      "4000 Train accarcy: 0.80524343\n",
      "4200 Train accarcy: 0.80898875\n",
      "4400 Train accarcy: 0.8227216\n",
      "4600 Train accarcy: 0.8164794\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4701\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X: train_prepared, y: train_survived, training: True})\n",
    "        accuracy_bat = accuracy.eval(feed_dict={X: train_prepared, y: train_survived})\n",
    "        if epoch % 200 == 0:\n",
    "            print(epoch, \"Train accarcy:\", accuracy_bat)\n",
    "    Z = logits.eval(feed_dict = {X: test_prepared})\n",
    "    y_pred =np.argmax(Z, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Submission File \n",
    "StackingSubmission = pd.DataFrame({'PassengerId': PassengerId,\n",
    "                                   'Survived': y_pred })\n",
    "StackingSubmission.to_csv(\"NN_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
